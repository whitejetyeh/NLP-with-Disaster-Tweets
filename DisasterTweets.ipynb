{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "DisasterTweets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eikXp-w0GVeI",
        "colab_type": "text"
      },
      "source": [
        "# 0. Outline\n",
        "\n",
        "This notebook is a short demonstration of applying the Bidirectional Encoder Representations from Transformers (BERT) from tensorflow hub for learning purpose. \n",
        "We will build an natural language processing (NLP) model to predict if a given tweet is talking about a real emergency situation or not. The data comes from a kaggle competition at \"getting started\" level, see https://www.kaggle.com/c/nlp-getting-started.\n",
        "\n",
        "The following sections started by very limited data exploration and light preprocessing, so we can quickly dive into the application of BERT. The first solution is to finetune BERT from pretrained weights in a few epochs of training. The second solution is to attach more dense layers after BERT while freezing BERT's weights. This way of transfer learning complexifies the model's structure for broader missions without heavy retraining. Finally, the third solution explores more models attached after BERT to further improve the predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmvvwWuCczLr",
        "colab_type": "text"
      },
      "source": [
        "Download datasets/models and install modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWf1A5-JhyDM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "3401ebc1-e040-4ecf-a3f3-dd6501a7da73"
      },
      "source": [
        "# pretrained BERT is downloaded from tensorflow hub by Google\n",
        "!wget https://storage.googleapis.com/tfhub-modules/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1.tar.gz\n",
        "!mkdir bert_model\n",
        "!tar -xvf '/content/1.tar.gz'  -C '/content/bert_model'\n",
        "module_path = \"/content/bert_model\"\n",
        "# direct url if one doesn't want to save the model\n",
        "#module_path = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
        "\n",
        "# download dataset provided by kaggle\n",
        "!wget --quiet https://raw.githubusercontent.com/whitejetyeh/NLP-with-Disaster-Tweets/master/train.csv\n",
        "!wget --quiet https://raw.githubusercontent.com/whitejetyeh/NLP-with-Disaster-Tweets/master/test.csv\n",
        "!wget --quiet https://raw.githubusercontent.com/whitejetyeh/NLP-with-Disaster-Tweets/master/sample_submission.csv\n",
        "\n",
        "# Download a text cleaning function for tweets\n",
        "#ref https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-full-cleaning\n",
        "!wget --quiet https://raw.githubusercontent.com/whitejetyeh/NLP-with-Disaster-Tweets/master/CleanTweets.py\n",
        "\n",
        "# the official tokenization script created by Google\n",
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
        "\n",
        "# for importing sentencepiece in tokenization.py\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-10 02:18:51--  https://storage.googleapis.com/tfhub-modules/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1.tar.gz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.219.128, 2607:f8b0:4001:c07::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.219.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1244531387 (1.2G) [application/x-tar]\n",
            "Saving to: ‘1.tar.gz’\n",
            "\n",
            "1.tar.gz            100%[===================>]   1.16G   109MB/s    in 8.9s    \n",
            "\n",
            "2020-02-10 02:19:00 (133 MB/s) - ‘1.tar.gz’ saved [1244531387/1244531387]\n",
            "\n",
            "./\n",
            "./variables/\n",
            "./variables/variables.data-00000-of-00001\n",
            "./variables/variables.index\n",
            "./assets/\n",
            "./assets/vocab.txt\n",
            "./saved_model.pb\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 3.3MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "C33bYTd5GVeQ",
        "colab_type": "code",
        "outputId": "a5cd1df2-dfe3-4426-f961-1e55be9202df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import tokenization\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "# check GPU connection with Google\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n",
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4h3i8plYJrN",
        "colab_type": "text"
      },
      "source": [
        "# 1. minimum data exploration and preprocessing\n",
        "\n",
        "Here, we slightly explore the dataset. More fine analysis can be found on kaggle, for example https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWEcudHziy6F",
        "colab_type": "text"
      },
      "source": [
        "## data exploration\n",
        "\n",
        "note: instead of reading test.csv for predicting unknown targets, we split a portion of train.csv to be df_test for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k66xeax1GVeu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "260f68f9-ef0f-48ea-90ca-21f2b7b49939"
      },
      "source": [
        "# read data prepared by kaggle into pandas' data frames\n",
        "df_train = pd.read_csv(\"/content/train.csv\")\n",
        "#df_test = pd.read_csv(\"/content/test.csv\")\n",
        "#submission = pd.read_csv(\"/content/sample_submission.csv\") #no need\n",
        "df_train, df_test = train_test_split(df_train,\n",
        "                                     test_size=0.1,\n",
        "                                     random_state=39)\n",
        "\n",
        "display(df_train.head())\n",
        "\n",
        "print('an example of a tweet')\n",
        "display(df_train.text.iloc[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4933</th>\n",
              "      <td>7027</td>\n",
              "      <td>mayhem</td>\n",
              "      <td>PG County, MD</td>\n",
              "      <td>Tonight It's Going To Be Mayhem @ #4PlayThursd...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1673</th>\n",
              "      <td>2416</td>\n",
              "      <td>collide</td>\n",
              "      <td>Kansas, The Free State! ~ KC</td>\n",
              "      <td>That sounds about right. Our building will hav...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5654</th>\n",
              "      <td>8066</td>\n",
              "      <td>rescue</td>\n",
              "      <td>Big NorthEast Litter Box</td>\n",
              "      <td>I'm on 2 blood pressure meds and it's still pr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3532</th>\n",
              "      <td>5049</td>\n",
              "      <td>eyewitness</td>\n",
              "      <td>Pennsylvania</td>\n",
              "      <td>A true #TBT  Eyewitness News WBRE WYOU http://...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5212</th>\n",
              "      <td>7444</td>\n",
              "      <td>obliterated</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I think I'll get obliterated tonight</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ... target\n",
              "4933  7027  ...      0\n",
              "1673  2416  ...      0\n",
              "5654  8066  ...      0\n",
              "3532  5049  ...      0\n",
              "5212  7444  ...      0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "an example of a tweet\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"Tonight It's Going To Be Mayhem @ #4PlayThursdays. Everybody Free w/ Text. 1716 I ST NW (18+) http://t.co/cQ7jJ6Yjfz\""
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpu7cw42e8xa",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "BERT makes predictions based on the text content in the 'text' column, and we will consider columns of 'keyword' and 'location' in the booster solution to improve BERT's predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZMWBnosf5Cf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d21bc4cb-61fb-41b6-bf87-37a1a71ca535"
      },
      "source": [
        "# basic infomation\n",
        "print('basic infomation of training data')\n",
        "display(df_train.info())\n",
        "print('basic infomation of test data')\n",
        "display(df_test.info())\n",
        "\n",
        "# stats of data\n",
        "print('stats of training data')\n",
        "display(df_train.describe(include=['object']))\n",
        "print('stats of test data')\n",
        "display(df_test.describe(include=['object']))\n",
        "\n",
        "# missing values\n",
        "print('missing values in the training data')\n",
        "display(df_train.isnull().sum())\n",
        "print('missing values in the test data')\n",
        "display(df_test.isnull().sum())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "basic infomation of training data\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 6851 entries, 4933 to 3465\n",
            "Data columns (total 5 columns):\n",
            "id          6851 non-null int64\n",
            "keyword     6797 non-null object\n",
            "location    4577 non-null object\n",
            "text        6851 non-null object\n",
            "target      6851 non-null int64\n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 321.1+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "basic infomation of test data\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 762 entries, 7313 to 3305\n",
            "Data columns (total 5 columns):\n",
            "id          762 non-null int64\n",
            "keyword     755 non-null object\n",
            "location    503 non-null object\n",
            "text        762 non-null object\n",
            "target      762 non-null int64\n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 35.7+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "stats of training data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6797</td>\n",
              "      <td>4577</td>\n",
              "      <td>6851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>221</td>\n",
              "      <td>3066</td>\n",
              "      <td>6764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>fatalities</td>\n",
              "      <td>USA</td>\n",
              "      <td>11-Year-Old Boy Charged With Manslaughter of T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>43</td>\n",
              "      <td>94</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           keyword location                                               text\n",
              "count         6797     4577                                               6851\n",
              "unique         221     3066                                               6764\n",
              "top     fatalities      USA  11-Year-Old Boy Charged With Manslaughter of T...\n",
              "freq            43       94                                                  9"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "stats of test data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>755</td>\n",
              "      <td>503</td>\n",
              "      <td>762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>212</td>\n",
              "      <td>440</td>\n",
              "      <td>760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>dust%20storm</td>\n",
              "      <td>USA</td>\n",
              "      <td>To fight bioterrorism sir.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>12</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             keyword location                        text\n",
              "count            755      503                         762\n",
              "unique           212      440                         760\n",
              "top     dust%20storm      USA  To fight bioterrorism sir.\n",
              "freq              12       10                           3"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "missing values in the training data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "id             0\n",
              "keyword       54\n",
              "location    2274\n",
              "text           0\n",
              "target         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "missing values in the test data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "id            0\n",
              "keyword       7\n",
              "location    259\n",
              "text          0\n",
              "target        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb5AvVQTKhhC",
        "colab_type": "text"
      },
      "source": [
        "## Text cleaning\n",
        "* The most common type of words in oov have punctuations at the start or end. Those words doesn't have embeddings because of the trailing punctuations. Punctuations #, @, !, ?, (, ),[, ], *, %, ..., ', ., :, ; are separated from words\n",
        "* Special characters that are attached to words are removed completely\n",
        "* Contractions are expanded\n",
        "* Urls are removed\n",
        "* Character entity references are replaced with their actual symbols\n",
        "* Typos and slang are corrected, and informal abbreviations are written in their long forms\n",
        "* Hashtags and usernames are expanded\n",
        "* Some words are replaced with their acronyms\n",
        "\n",
        "See https://raw.githubusercontent.com/whitejetyeh/NLP-with-Disaster-Tweets/master/CleanTweets.py for details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N_uvuUk8fae",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "2ea1ee16-81fb-416d-ab58-aceb99c6b4a0"
      },
      "source": [
        "def clean(tweet): \n",
        "            \n",
        "    # Special characters\n",
        "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
        "    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
        "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
        "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n",
        "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
        "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
        "    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
        "    tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
        "    \n",
        "    # Contractions\n",
        "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
        "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
        "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
        "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
        "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
        "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
        "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
        "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
        "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
        "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
        "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
        "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
        "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
        "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
        "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
        "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
        "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
        "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
        "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
        "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
        "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
        "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
        "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
        "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
        "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
        "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
        "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
        "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
        "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
        "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
        "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
        "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
        "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
        "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
        "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
        "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
        "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
        "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
        "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
        "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
        "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
        "    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
        "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n",
        "            \n",
        "    # Character entity references\n",
        "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
        "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
        "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
        "    \n",
        "    # Typos, slang and informal abbreviations\n",
        "    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
        "    tweet = re.sub(r\"w/\", \"with\", tweet)\n",
        "    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n",
        "    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n",
        "    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n",
        "    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n",
        "    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n",
        "    tweet = re.sub(r\"<3\", \"love\", tweet)\n",
        "    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n",
        "    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n",
        "    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n",
        "    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n",
        "    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n",
        "    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n",
        "    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n",
        "    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n",
        "    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n",
        "    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n",
        "    \n",
        "    # Hashtags and usernames\n",
        "    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n",
        "    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n",
        "    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n",
        "    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n",
        "    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n",
        "    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n",
        "    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n",
        "    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n",
        "    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n",
        "    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n",
        "    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n",
        "    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n",
        "    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n",
        "    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n",
        "    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n",
        "    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n",
        "    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n",
        "    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n",
        "    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n",
        "    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n",
        "    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n",
        "    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n",
        "    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n",
        "    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n",
        "    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n",
        "    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n",
        "    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n",
        "    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n",
        "    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n",
        "    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n",
        "    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n",
        "    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n",
        "    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n",
        "    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n",
        "    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n",
        "    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n",
        "    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n",
        "    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n",
        "    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n",
        "    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n",
        "    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n",
        "    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n",
        "    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n",
        "    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n",
        "    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n",
        "    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n",
        "    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n",
        "    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n",
        "    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n",
        "    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n",
        "    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n",
        "    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n",
        "    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n",
        "    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n",
        "    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n",
        "    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n",
        "    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n",
        "    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n",
        "    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n",
        "    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n",
        "    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n",
        "    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n",
        "    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n",
        "    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \n",
        "    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\n",
        "    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n",
        "    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n",
        "    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n",
        "    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n",
        "    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n",
        "    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n",
        "    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n",
        "    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n",
        "    tweet = re.sub(r\"OBLITERATION\", \"obliteration\", tweet)\n",
        "    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n",
        "    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n",
        "    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n",
        "    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n",
        "    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n",
        "    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n",
        "    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n",
        "    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n",
        "    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n",
        "    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n",
        "    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n",
        "    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n",
        "    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n",
        "    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n",
        "    tweet = re.sub(r\"megynkelly\", \"Megyn Kelly\", tweet)\n",
        "    tweet = re.sub(r\"cnewslive\", \"C News Live\", tweet)\n",
        "    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n",
        "    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n",
        "    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n",
        "    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n",
        "    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n",
        "    tweet = re.sub(r\"cjoyner\", \"Chris Joyner\", tweet)\n",
        "    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n",
        "    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n",
        "    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n",
        "    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n",
        "    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n",
        "    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n",
        "    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n",
        "    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n",
        "    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n",
        "    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n",
        "    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n",
        "    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n",
        "    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n",
        "    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n",
        "    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n",
        "    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n",
        "    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n",
        "    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n",
        "    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n",
        "    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n",
        "    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n",
        "    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n",
        "    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n",
        "    tweet = re.sub(r\"S3XLEAK\", \"sex leak\", tweet)\n",
        "    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n",
        "    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n",
        "    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n",
        "    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n",
        "    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n",
        "    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n",
        "    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n",
        "    tweet = re.sub(r\"foodscare\", \"food scare\", tweet)\n",
        "    tweet = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", tweet)\n",
        "    tweet = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", tweet)\n",
        "    tweet = re.sub(r\"GamerGate\", \"Gamer Gate\", tweet)\n",
        "    tweet = re.sub(r\"IHHen\", \"Humanitarian Relief\", tweet)\n",
        "    tweet = re.sub(r\"spinningbot\", \"spinning bot\", tweet)\n",
        "    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n",
        "    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n",
        "    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n",
        "    tweet = re.sub(r\"po_st\", \"po.st\", tweet)\n",
        "    tweet = re.sub(r\"scoopit\", \"scoop.it\", tweet)\n",
        "    tweet = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", tweet)\n",
        "    tweet = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", tweet)\n",
        "    tweet = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", tweet)\n",
        "    tweet = re.sub(r\"rapidcity\", \"Rapid City\", tweet)\n",
        "    tweet = re.sub(r\"OutBid\", \"outbid\", tweet)\n",
        "    tweet = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", tweet)\n",
        "    tweet = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", tweet)\n",
        "    tweet = re.sub(r\"15PM\", \"15 PM\", tweet)\n",
        "    tweet = re.sub(r\"OriginalFunko\", \"Funko\", tweet)\n",
        "    tweet = re.sub(r\"rightwaystan\", \"Richard Tan\", tweet)\n",
        "    tweet = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", tweet)\n",
        "    tweet = re.sub(r\"RT_America\", \"RT America\", tweet)\n",
        "    tweet = re.sub(r\"narendramodi\", \"Narendra Modi\", tweet)\n",
        "    tweet = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", tweet)\n",
        "    tweet = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", tweet)\n",
        "    tweet = re.sub(r\"alexbelloli\", \"Alex Belloli\", tweet)\n",
        "    tweet = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", tweet)\n",
        "    tweet = re.sub(r\"gunsense\", \"gun sense\", tweet)\n",
        "    tweet = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", tweet)\n",
        "    tweet = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", tweet)\n",
        "    tweet = re.sub(r\"samanthaturne19\", \"Samantha Turner\", tweet)\n",
        "    tweet = re.sub(r\"JonVoyage\", \"Jon Stewart\", tweet)\n",
        "    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\n",
        "    tweet = re.sub(r\"SuryaRay\", \"Surya Ray\", tweet)\n",
        "    tweet = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", tweet)\n",
        "    tweet = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", tweet)\n",
        "    tweet = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", tweet)\n",
        "    tweet = re.sub(r\"pmarca\", \"Marc Andreessen\", tweet)\n",
        "    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\n",
        "    tweet = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", tweet)\n",
        "    tweet = re.sub(r\"Japton\", \"Arkansas\", tweet)\n",
        "    tweet = re.sub(r\"RouteComplex\", \"Route Complex\", tweet)\n",
        "    tweet = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", tweet)\n",
        "    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\n",
        "    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\n",
        "    tweet = re.sub(r\"Hiroshima70\", \"Hiroshima\", tweet)\n",
        "    tweet = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", tweet)\n",
        "    tweet = re.sub(r\"versethe\", \"verse the\", tweet)\n",
        "    tweet = re.sub(r\"TubeStrike\", \"Tube Strike\", tweet)\n",
        "    tweet = re.sub(r\"MissionHills\", \"Mission Hills\", tweet)\n",
        "    tweet = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", tweet)\n",
        "    tweet = re.sub(r\"NANKANA\", \"Nankana\", tweet)\n",
        "    tweet = re.sub(r\"SAHIB\", \"Sahib\", tweet)\n",
        "    tweet = re.sub(r\"PAKPATTAN\", \"Pakpattan\", tweet)\n",
        "    tweet = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", tweet)\n",
        "    tweet = re.sub(r\"gofundme\", \"go fund me\", tweet)\n",
        "    tweet = re.sub(r\"pmharper\", \"Stephen Harper\", tweet)\n",
        "    tweet = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", tweet)\n",
        "    tweet = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", tweet)\n",
        "    tweet = re.sub(r\"bancodeseries\", \"banco de series\", tweet)\n",
        "    tweet = re.sub(r\"timkaine\", \"Tim Kaine\", tweet)\n",
        "    tweet = re.sub(r\"IdentityTheft\", \"Identity Theft\", tweet)\n",
        "    tweet = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", tweet)\n",
        "    tweet = re.sub(r\"mishacollins\", \"Misha Collins\", tweet)\n",
        "    tweet = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", tweet)\n",
        "    tweet = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", tweet)\n",
        "    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\n",
        "    tweet = re.sub(r\"ScreamQueens\", \"Scream Queens\", tweet)\n",
        "    tweet = re.sub(r\"AskCharley\", \"Ask Charley\", tweet)\n",
        "    tweet = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", tweet)\n",
        "    tweet = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", tweet)\n",
        "    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\n",
        "    tweet = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", tweet)\n",
        "    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\n",
        "    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\n",
        "    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\n",
        "    tweet = re.sub(r\"savebees\", \"save bees\", tweet)\n",
        "    tweet = re.sub(r\"GreenHarvard\", \"Green Harvard\", tweet)\n",
        "    tweet = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", tweet)\n",
        "    tweet = re.sub(r\"hermancranston\", \"Herman Cranston\", tweet)\n",
        "    tweet = re.sub(r\"WMUR9\", \"WMUR-TV\", tweet)\n",
        "    tweet = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", tweet)\n",
        "    tweet = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", tweet)\n",
        "    tweet = re.sub(r\"ProSyn\", \"Project Syndicate\", tweet)\n",
        "    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\n",
        "    tweet = re.sub(r\"s2g\", \"swear to god\", tweet)\n",
        "    tweet = re.sub(r\"listenlive\", \"listen live\", tweet)\n",
        "    tweet = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", tweet)\n",
        "    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\n",
        "    tweet = re.sub(r\"CBSBigBrother\", \"Big Brother\", tweet)\n",
        "    tweet = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", tweet)\n",
        "    tweet = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", tweet)\n",
        "    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", tweet)\n",
        "    tweet = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", tweet)\n",
        "    tweet = re.sub(r\"Popularmmos\", \"Popular MMOs\", tweet)\n",
        "    tweet = re.sub(r\"WildHorses\", \"Wild Horses\", tweet)\n",
        "    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\n",
        "    tweet = re.sub(r\"HORNDALE\", \"Horndale\", tweet)\n",
        "    tweet = re.sub(r\"PINER\", \"Piner\", tweet)\n",
        "    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\n",
        "    tweet = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", tweet)\n",
        "    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\n",
        "    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\n",
        "    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\n",
        "    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\n",
        "    tweet = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", tweet)\n",
        "    tweet = re.sub(r\"shoalstraffic\", \"shoals traffic\", tweet)\n",
        "    tweet = re.sub(r\"GeorgeFoster72\", \"George Foster\", tweet)\n",
        "    tweet = re.sub(r\"pop2015\", \"pop 2015\", tweet)\n",
        "    tweet = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", tweet)\n",
        "    tweet = re.sub(r\"DianneG\", \"Dianne Gallagher\", tweet)\n",
        "    tweet = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", tweet)\n",
        "    tweet = re.sub(r\"BritishBakeOff\", \"British Bake Off\", tweet)\n",
        "    tweet = re.sub(r\"FreeKashmir\", \"Free Kashmir\", tweet)\n",
        "    tweet = re.sub(r\"mattmosley\", \"Matt Mosley\", tweet)\n",
        "    tweet = re.sub(r\"BishopFred\", \"Bishop Fred\", tweet)\n",
        "    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\n",
        "    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\n",
        "    tweet = re.sub(r\"UNHEALED\", \"unhealed\", tweet)\n",
        "    tweet = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", tweet)\n",
        "    tweet = re.sub(r\"Latestnews\", \"Latest news\", tweet)\n",
        "    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\n",
        "    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\n",
        "    tweet = re.sub(r\"datingtips\", \"dating tips\", tweet)\n",
        "    tweet = re.sub(r\"charlesadler\", \"Charles Adler\", tweet)\n",
        "    tweet = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", tweet)\n",
        "    tweet = re.sub(r\"txlege\", \"Texas Legislature\", tweet)\n",
        "    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\n",
        "           \n",
        "    # Urls\n",
        "    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n",
        "        \n",
        "    # Words with punctuations and special characters\n",
        "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
        "    for p in punctuations:\n",
        "        tweet = tweet.replace(p, f' {p} ')\n",
        "        \n",
        "    # ... and ..\n",
        "    tweet = tweet.replace('...', ' ... ')\n",
        "    if '...' not in tweet:\n",
        "        tweet = tweet.replace('..', ' ... ')      \n",
        "        \n",
        "    # Acronyms\n",
        "    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n",
        "    tweet = re.sub(r\"mÌ¼sica\", \"music\", tweet)\n",
        "    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n",
        "    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n",
        "    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n",
        "    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n",
        "    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n",
        "    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n",
        "    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)    \n",
        "    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n",
        "    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n",
        "    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)   \n",
        "    \n",
        "    # Grouping same words without embeddings\n",
        "    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\n",
        "    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\n",
        "    \n",
        "    return tweet\n",
        "\n",
        "df_train['text_cleaned'] = df_train['text'].apply(lambda s : clean(s))\n",
        "df_test['text_cleaned'] = df_test['text'].apply(lambda s : clean(s))\n",
        "\n",
        "print('a tweet before cleaning')\n",
        "display(df_train.text.iloc[0])\n",
        "print('a tweet after cleaning')\n",
        "display(df_train.text_cleaned.iloc[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a tweet before cleaning\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"Tonight It's Going To Be Mayhem @ #4PlayThursdays. Everybody Free w/ Text. 1716 I ST NW (18+) http://t.co/cQ7jJ6Yjfz\""
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "a tweet after cleaning\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Tonight It is Going To Be Mayhem  @   # Foreplay Thursdays .  Everybody Free with Text .  1716 I ST NW  ( 18 +  )  '"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UhGzBkBKidj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "8996cf5b-5f23-4ccb-c6cf-64e313474565"
      },
      "source": [
        "# clean is a handmade text cleaning function for tweets\n",
        "#Reference: https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-full-cleaning\n",
        "from CleanTweets import clean\n",
        "df_train['text_cleaned'] = df_train['text'].apply(lambda s : clean(s))\n",
        "df_test['text_cleaned'] = df_test['text'].apply(lambda s : clean(s))\n",
        "\n",
        "print('a tweet before cleaning')\n",
        "display(df_train.text.iloc[0])\n",
        "print('a tweet after cleaning')\n",
        "display(df_train.text_cleaned.iloc[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d6f0edc36946>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mCleanTweets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_cleaned'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_cleaned'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a tweet before cleaning'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4043\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4044\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4045\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4047\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-d6f0edc36946>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mCleanTweets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_cleaned'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_cleaned'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a tweet before cleaning'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CleanTweets.py\u001b[0m in \u001b[0;36mclean\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Special characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\x89Û_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELAoxFXMYUS3",
        "colab_type": "text"
      },
      "source": [
        "# 2. finetune a pretrained BERT model\n",
        "Here, we apply the plain BERT model. The first step is to process text data into the BERT format [token, mask, segment] with `bert_encode` and `tokenizer`. Then, the model defined in `build_model` is the original base BERT with the one node output layer determining predictions to be 1 for real disaster or 0 for non-disaster.\n",
        "\n",
        "This part is forked from another fine kernel on kaggle, \n",
        "https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB-rgQe1p_GM",
        "colab_type": "text"
      },
      "source": [
        "## define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLpE6T7rGVea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bert_encode(texts, tokenizer, max_len=512):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    \n",
        "    for text in texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "            \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len - len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
        "        tokens += [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "    \n",
        "    return [np.array(all_tokens), np.array(all_masks), np.array(all_segments)]\n",
        "\n",
        "def build_model(bert_layer, max_len=512):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    out = Dense(1, activation='sigmoid')(clf_output)\n",
        "    \n",
        "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g-IoRAKp0jo",
        "colab_type": "text"
      },
      "source": [
        "## initialize BERT and encode text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-tB-ZYEpjx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize BERT with trainable weights\n",
        "bert_layer = hub.KerasLayer(module_path, trainable=True)\n",
        "\n",
        "# establish tokenizer with bert_layer\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "\n",
        "# encode cleaned text data for BERT to read\n",
        "# the maximum text length (max_len) with BERT-base is 512\n",
        "train_input = bert_encode(df_train.text_cleaned.values, tokenizer, max_len=160)\n",
        "test_input = bert_encode(df_test.text_cleaned.values, tokenizer, max_len=160)\n",
        "train_labels = df_train.target.values\n",
        "test_labels = df_test.target.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aT0HzVXGVe8",
        "colab_type": "text"
      },
      "source": [
        "## train model and predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXuD-KO-q5EK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "outputId": "c6f736d4-0c71-4f46-acf0-f3061b5e54f2"
      },
      "source": [
        "%%time\n",
        "\n",
        "train_model = False\n",
        "if train_model:\n",
        "    # initialize model\n",
        "    n_epoch = 3\n",
        "    model = build_model(bert_layer, max_len=160)\n",
        "    checkpoint_path = \"/content/bert_model.ckpt\"\n",
        "    display(model.summary())\n",
        "\n",
        "    # start training (about 30 mins)\n",
        "    # Create a callback that saves the model's weights\n",
        "    cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                  save_weights_only=True,\n",
        "                                  save_best_only=True,\n",
        "                                  verbose=1)\n",
        "    model.fit(train_input, train_labels,\n",
        "              validation_split=0.1,\n",
        "              epochs=n_epoch,\n",
        "              batch_size=16,\n",
        "              callbacks=[cp_callback])\n",
        "\n",
        "    # predict df_test (validation data from train.csv)\n",
        "    predictions = model.predict(test_input).round().astype(int)\n",
        "    print(classification_report(test_labels, predictions, labels=[0, 1]))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 160)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 160)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 160)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            1025        tf_op_layer_strided_slice[0][0]  \n",
            "==================================================================================================\n",
            "Total params: 335,142,914\n",
            "Trainable params: 335,142,913\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 6165 samples, validate on 686 samples\n",
            "Epoch 1/3\n",
            "6160/6165 [============================>.] - ETA: 0s - loss: 0.4842 - accuracy: 0.7771\n",
            "Epoch 00001: val_loss improved from inf to 0.43643, saving model to /content/bert_model.ckpt\n",
            "6165/6165 [==============================] - 742s 120ms/sample - loss: 0.4844 - accuracy: 0.7771 - val_loss: 0.4364 - val_accuracy: 0.8105\n",
            "Epoch 2/3\n",
            "6160/6165 [============================>.] - ETA: 0s - loss: 0.3424 - accuracy: 0.8589\n",
            "Epoch 00002: val_loss improved from 0.43643 to 0.42187, saving model to /content/bert_model.ckpt\n",
            "6165/6165 [==============================] - 708s 115ms/sample - loss: 0.3426 - accuracy: 0.8589 - val_loss: 0.4219 - val_accuracy: 0.8090\n",
            "Epoch 3/3\n",
            "6160/6165 [============================>.] - ETA: 0s - loss: 0.2672 - accuracy: 0.8927\n",
            "Epoch 00003: val_loss did not improve from 0.42187\n",
            "6165/6165 [==============================] - 634s 103ms/sample - loss: 0.2673 - accuracy: 0.8926 - val_loss: 0.4861 - val_accuracy: 0.7930\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       451\n",
            "           1       0.81      0.77      0.79       311\n",
            "\n",
            "    accuracy                           0.83       762\n",
            "   macro avg       0.83      0.82      0.83       762\n",
            "weighted avg       0.83      0.83      0.83       762\n",
            "\n",
            "CPU times: user 16min 53s, sys: 11min 42s, total: 28min 35s\n",
            "Wall time: 35min 16s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SnqY-Zqxdqx",
        "colab_type": "text"
      },
      "source": [
        "## load trained model for inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P3jvMbNwXRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "load_model = False\n",
        "if load_model:\n",
        "    # load trained model for prediction\n",
        "    model_path = \"/content/bert_model.ckpt\"\n",
        "\n",
        "    # initialize model with the same structure as the loaded one\n",
        "    model = build_model(bert_layer, max_len=160)\n",
        "    display(model.summary())\n",
        "    # Load the previously saved weights\n",
        "    model.load_weights(model_path)\n",
        "\n",
        "    # predict df_test (validation data from train.csv)\n",
        "    predictions = model.predict(test_input)\n",
        "    print(classification_report(test_labels, predictions, labels=[0, 1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5uxziJWYjlx",
        "colab_type": "text"
      },
      "source": [
        "# 3. transfer learning with BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNS1J0VxsN2",
        "colab_type": "text"
      },
      "source": [
        "## define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JvvlMwNkFTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Transfer learning of bert'''\n",
        "def build_ext_model(module_path, max_len=512):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "    # load BERT with frozen weights\n",
        "    bert_layer = hub.KerasLayer(module_path, trainable=False)\n",
        "\n",
        "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    # dense layers stacked after bert\n",
        "    x = tf.keras.layers.Dense(400, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(0.1)(x)\n",
        "    x = tf.keras.layers.Dense(200, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(0.1)(x)\n",
        "    x = tf.keras.layers.Dense(100, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(0.1)(x)\n",
        "    out = Dense(1, activation='sigmoid', name=\"dense_output\")(x)\n",
        "    \n",
        "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "    model.compile(Adam(lr=3e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6ZO81fqxxYW",
        "colab_type": "text"
      },
      "source": [
        "## train model and predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4qVJvdum5jS",
        "colab_type": "code",
        "outputId": "281464b2-b2de-4e36-e863-9f8fecfca64d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "train_model = False\n",
        "if train_model:\n",
        "    # initialize model\n",
        "    n_epoch = 5\n",
        "    model = build_ext_model(module_path, max_len=160)    \n",
        "    checkpoint_path = \"/content/bert_ext_model.ckpt\"\n",
        "    display(model.summary())\n",
        "\n",
        "    # start training (about 30 mins)\n",
        "    # Create a callback that saves the model's weights\n",
        "    cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                  save_weights_only=True,\n",
        "                                  save_best_only=True,\n",
        "                                  verbose=1)\n",
        "    model.fit(train_input, train_labels,\n",
        "              validation_split=0.1,\n",
        "              epochs=n_epoch,\n",
        "              batch_size=16,\n",
        "              callbacks=[cp_callback])\n",
        "\n",
        "    # predict df_test (validation data from train.csv)\n",
        "    predictions = model.predict(test_input).round().astype(int)\n",
        "    print(classification_report(test_labels, predictions, labels=[0, 1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87       294\n",
            "           1       0.82      0.82      0.82       206\n",
            "\n",
            "    accuracy                           0.85       500\n",
            "   macro avg       0.85      0.85      0.85       500\n",
            "weighted avg       0.85      0.85      0.85       500\n",
            "\n",
            "CPU times: user 14.3 s, sys: 9.03 s, total: 23.3 s\n",
            "Wall time: 50.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RneORUPc1zPe",
        "colab_type": "text"
      },
      "source": [
        "## load trained model for inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJMLCwHn1zjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "load_model = False\n",
        "if load_model:\n",
        "    # load trained model for prediction\n",
        "    model_path = \"/content/bert_ext_model.ckpt\"\n",
        "\n",
        "    # initialize model with the same structure as the loaded one\n",
        "    model = build_model(bert_layer, max_len=160)\n",
        "    display(model.summary())\n",
        "    # Load the previously saved weights\n",
        "    model.load_weights(model_path)\n",
        "\n",
        "    # predict df_test (validation data from train.csv)\n",
        "    predictions = model.predict(test_input)\n",
        "    print(classification_report(test_labels, predictions, labels=[0, 1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv9-gGSh28mr",
        "colab_type": "text"
      },
      "source": [
        "# MY GOOGLE DRIVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sOQQXyRHGh9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "2ac55501-5199-40a5-96fe-ed839299bb6c"
      },
      "source": [
        "!ls -l /content/"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 5144792\n",
            "-rw-r--r-- 1 root root 1244531387 Oct 22 10:28 1.tar.gz\n",
            "drwxr-x--T 4 root root       4096 Oct 22 08:20 bert_model\n",
            "-rw-r--r-- 1 root root     410812 Feb 10 03:01 bert_model.ckpt.data-00000-of-00002\n",
            "-rw-r--r-- 1 root root 4021714981 Feb 10 03:02 bert_model.ckpt.data-00001-of-00002\n",
            "-rw-r--r-- 1 root root      90631 Feb 10 03:02 bert_model.ckpt.index\n",
            "-rw-r--r-- 1 root root         87 Feb 10 03:02 checkpoint\n",
            "-rw-r--r-- 1 root root      23231 Feb 10 02:34 CleanTweets.py\n",
            "drwxr-xr-x 2 root root       4096 Feb 10 02:29 __pycache__\n",
            "drwxr-xr-x 1 root root       4096 Feb  5 18:37 sample_data\n",
            "-rw-r--r-- 1 root root      22746 Feb 10 02:19 sample_submission.csv\n",
            "-rw-r--r-- 1 root root     420783 Feb 10 02:19 test.csv\n",
            "-rw-r--r-- 1 root root      16775 Feb 10 02:19 tokenization.py\n",
            "-rw-r--r-- 1 root root     987712 Feb 10 02:19 train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foSRwlCrGVfN",
        "colab_type": "code",
        "outputId": "965f7bbe-7832-47c1-9752-3cb7bb83931d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "download_weight = True\n",
        "if download_weight:\n",
        "    !cp -r /gdrive/'My Drive'/kaggletest/. /content/\n",
        "\n",
        "upload_bert = False\n",
        "if upload_bert:\n",
        "    !cp -r /content/bert_model.ckpt* /gdrive/'My Drive'/kaggletest/\n",
        "    #!cp -r /content/bert_ext_model.ckpt* /gdrive/'My Drive'/kaggletest/"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb4ASvWd47pY",
        "colab_type": "text"
      },
      "source": [
        "# 3. subsequent booster after BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3JWMXEov8c8",
        "colab_type": "text"
      },
      "source": [
        "## encode categorical features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JSenH5Gv7qw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''encode categorical features'''\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from collections import defaultdict\n",
        "\n",
        "cat_df_train = df_train[['keyword','location']].copy()\n",
        "cat_df_test = df_test[['keyword','location']].copy()\n",
        "# handle missing values\n",
        "for cat in ['keyword','location']:\n",
        "    cat_df_train[cat].loc[cat_df_train[cat].isnull()] = 'NaN'\n",
        "    cat_df_test[cat].loc[cat_df_test[cat].isnull()] = 'NaN'\n",
        "\n",
        "# initialize the encoder\n",
        "le = defaultdict(LabelEncoder)\n",
        "# fit the encoder and transform the training set\n",
        "fit_cat_df_train = cat_df_train.apply(lambda x: le[x.name].fit_transform(x))\n",
        "# normalize the encoding\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "fit_cat_df_train = scaler.fit_transform(fit_cat_df_train)\n",
        "fit_cat_df_train = pd.DataFrame(fit_cat_df_train,\n",
        "                                index=cat_df_train.index,\n",
        "                                columns=['keyword','location'])\n",
        "\n",
        "\n",
        "# Replace test set labels unseen in the training set\n",
        "for cat in ['keyword','location']:\n",
        "    labels_train = cat_df_train[cat].unique().tolist()\n",
        "    replacement_label = cat_df_train[cat].mode()[0]\n",
        "    cat_df_test[cat].loc[~cat_df_test[cat].isin(labels_train)] = replacement_label\n",
        "\n",
        "# Using the dictionary to label future data\n",
        "fit_cat_df_test = cat_df_test.apply(lambda x: le[x.name].transform(x))\n",
        "# normalize the encoding\n",
        "fit_cat_df_test = scaler.transform(fit_cat_df_test)\n",
        "fit_cat_df_test = pd.DataFrame(fit_cat_df_test,\n",
        "                               index=cat_df_test.index,\n",
        "                               columns=['keyword','location'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSQWfbZ6yw-5",
        "colab_type": "text"
      },
      "source": [
        "## aggregate bert predictions with extra features in subsequent models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dPVZ1VHy5au",
        "colab_type": "code",
        "outputId": "6d1f225a-a280-4eea-ffd4-049fc840dbc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "%%time\n",
        "'''aggregate previous predictions with extra features(processed)'''\n",
        "\n",
        "# preliminary prediction from BERT\n",
        "bert_train_pred = model.predict(train_input)\n",
        "bert_predict_df = pd.DataFrame(bert_train_pred, \n",
        "                               index=df_train.index,\n",
        "                               columns=['target'])\n",
        "# more features to be considered\n",
        "boosting_input = pd.concat([bert_predict_df,fit_cat_df_train],axis=1)\n",
        "    \n",
        "bert_test_pred = model.predict(test_input)\n",
        "bert_test_predict_df = pd.DataFrame(bert_test_pred, \n",
        "                                    index=df_test.index,\n",
        "                                    columns=['target'])\n",
        "# more features to be considered\n",
        "test_boosting_input = pd.concat([bert_test_predict_df,fit_cat_df_test],axis=1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-a125e74cfe49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"'''aggregate previous predictions with extra features(processed)'''\\n\\n# preliminary prediction from BERT\\nbert_train_pred = model.predict(train_input)\\nbert_predict_df = pd.DataFrame(bert_train_pred, \\n                               index=df_train.index,\\n                               columns=['target'])\\n# more features to be considered\\nboosting_input = pd.concat([bert_predict_df,fit_cat_df_train],axis=1)\\n    \\nbert_test_pred = model.predict(test_input)\\nbert_test_predict_df = pd.DataFrame(bert_test_pred, \\n                                    index=df_test.index,\\n                                    columns=['target'])\\n# more features to be considered\\ntest_boosting_input = pd.concat([test_bert_predict_df,fit_cat_df_test],axis=1)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_bert_predict_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7POrWiT_AJj",
        "colab_type": "code",
        "outputId": "def5296e-716c-4270-e29d-1e88080eadd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "%%time\n",
        "'''HistGradientBoosting Classifier (lightGBM inspired)'''\n",
        "# To use this experimental feature, we need to explicitly ask for it:\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "HGB_classifier = HistGradientBoostingClassifier()\n",
        "HGB_classifier.fit(boosting_input, train_labels)\n",
        "\n",
        "predictions = HGB_classifier.predict(test_boosting_input).round().astype(int)\n",
        "print(classification_report(test_labels, predictions, labels=[0, 1]))\n",
        "print(HGB_classifier)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       451\n",
            "           1       0.80      0.78      0.79       311\n",
            "\n",
            "    accuracy                           0.83       762\n",
            "   macro avg       0.83      0.82      0.82       762\n",
            "weighted avg       0.83      0.83      0.83       762\n",
            "\n",
            "HistGradientBoostingClassifier(l2_regularization=0.0, learning_rate=0.1,\n",
            "                               loss='auto', max_bins=255, max_depth=None,\n",
            "                               max_iter=100, max_leaf_nodes=31,\n",
            "                               min_samples_leaf=20, n_iter_no_change=None,\n",
            "                               random_state=None, scoring=None, tol=1e-07,\n",
            "                               validation_fraction=0.1, verbose=0,\n",
            "                               warm_start=False)\n",
            "CPU times: user 652 ms, sys: 22.4 ms, total: 675 ms\n",
            "Wall time: 356 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9BwRI0ZAF-A",
        "colab_type": "code",
        "outputId": "27d915a3-7cd8-44d5-dfd0-b14bd2cc4342",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "%%time\n",
        "'''XGBoost'''\n",
        "import xgboost as xgb\n",
        "XGB_classifier = xgb.XGBClassifier()\n",
        "XGB_classifier.fit(boosting_input, train_labels)\n",
        "\n",
        "predictions = XGB_classifier.predict(test_boosting_input).round().astype(int)\n",
        "print(classification_report(test_labels, predictions, labels=[0, 1]))\n",
        "print(XGB_classifier)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       451\n",
            "           1       0.80      0.78      0.79       311\n",
            "\n",
            "    accuracy                           0.83       762\n",
            "   macro avg       0.83      0.82      0.82       762\n",
            "weighted avg       0.83      0.83      0.83       762\n",
            "\n",
            "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
            "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
            "              nthread=None, objective='binary:logistic', random_state=0,\n",
            "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "              silent=None, subsample=1, verbosity=1)\n",
            "CPU times: user 276 ms, sys: 55.1 ms, total: 331 ms\n",
            "Wall time: 1.01 s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}