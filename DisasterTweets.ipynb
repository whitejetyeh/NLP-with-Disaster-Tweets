{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"DisasterTweets.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"eikXp-w0GVeI","colab_type":"text"},"source":["# 0. Outline\n","\n","This notebook is a short demonstration of applying the Bidirectional Encoder Representations from Transformers (BERT) from tensorflow hub for learning purpose. \n","We will build an natural language processing (NLP) model to predict if a given tweet is talking about a real emergency situation or not. The data comes from a kaggle competition at \"getting started\" level, see https://www.kaggle.com/c/nlp-getting-started.\n","\n","The following sections started by very limited data exploration and light preprocessing, so we can quickly dive into the application of BERT. The first solution is to finetune BERT from pretrained weights in a few epochs of training. The second solution is to attach more dense layers after BERT while freezing BERT's weights. This way of transfer learning complexifies the model's structure for broader missions without heavy retraining. Finally, the third solution explores more models attached after BERT to further improve the predictions.\n"]},{"cell_type":"markdown","metadata":{"id":"kmvvwWuCczLr","colab_type":"text"},"source":["Download datasets/models and install modules"]},{"cell_type":"code","metadata":{"id":"RWf1A5-JhyDM","colab_type":"code","outputId":"ae01ca0c-6908-42b0-85c8-94800d4de429","executionInfo":{"status":"ok","timestamp":1581352543688,"user_tz":240,"elapsed":63186,"user":{"displayName":"Yeh Ken Huai-Che","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAf2gHx8A9v2eLa5kx_yhYDpU-RovU0raLpp45QKQ=s64","userId":"07065281398709988421"}},"colab":{"base_uri":"https://localhost:8080/","height":436}},"source":["# pretrained BERT is downloaded from tensorflow hub by Google\n","!wget https://storage.googleapis.com/tfhub-modules/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1.tar.gz\n","!mkdir bert_model\n","!tar -xvf '/content/1.tar.gz'  -C '/content/bert_model'\n","module_path = \"/content/bert_model\"\n","# direct url if one doesn't want to save the model\n","#module_path = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n","\n","# download dataset provided by kaggle\n","!wget --quiet https://raw.githubusercontent.com/whitejetyeh/NLP-with-Disaster-Tweets/master/train.csv\n","!wget --quiet https://raw.githubusercontent.com/whitejetyeh/NLP-with-Disaster-Tweets/master/test.csv\n","!wget --quiet https://raw.githubusercontent.com/whitejetyeh/NLP-with-Disaster-Tweets/master/sample_submission.csv\n","\n","# Download a text cleaning function for tweets\n","#ref https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-full-cleaning\n","!wget --quiet https://raw.githubusercontent.com/whitejetyeh/NLP-with-Disaster-Tweets/master/CleanTweets.py\n","\n","# the official tokenization script created by Google\n","!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n","\n","# for importing sentencepiece in tokenization.py\n","!pip install sentencepiece"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-02-10 16:34:40--  https://storage.googleapis.com/tfhub-modules/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1.tar.gz\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.143.128, 2a00:1450:4013:c01::80\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.143.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1244531387 (1.2G) [application/x-tar]\n","Saving to: ‘1.tar.gz’\n","\n","1.tar.gz            100%[===================>]   1.16G  56.6MB/s    in 18s     \n","\n","2020-02-10 16:35:03 (66.1 MB/s) - ‘1.tar.gz’ saved [1244531387/1244531387]\n","\n","./\n","./variables/\n","./variables/variables.data-00000-of-00001\n","./variables/variables.index\n","./assets/\n","./assets/vocab.txt\n","./saved_model.pb\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 9.2MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.85\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"C33bYTd5GVeQ","colab_type":"code","outputId":"c48647d8-73cc-4ca8-9ff6-0cf3853fad9d","executionInfo":{"status":"ok","timestamp":1581352552605,"user_tz":240,"elapsed":65977,"user":{"displayName":"Yeh Ken Huai-Che","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAf2gHx8A9v2eLa5kx_yhYDpU-RovU0raLpp45QKQ=s64","userId":"07065281398709988421"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["%tensorflow_version 2.x\n","import re\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","import tensorflow_hub as hub\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import tokenization\n","\n","print(tf.__version__)\n","\n","# check GPU connection with Google\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n","2.1.0\n","Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"g4h3i8plYJrN","colab_type":"text"},"source":["# 1. minimum data exploration and preprocessing\n","\n","Here, we slightly explore the dataset. More fine analysis can be found on kaggle, for example https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert."]},{"cell_type":"markdown","metadata":{"id":"PWEcudHziy6F","colab_type":"text"},"source":["## data exploration\n","\n","note: instead of reading test.csv for predicting unknown targets, we split a portion of train.csv to be df_test for validation."]},{"cell_type":"code","metadata":{"id":"k66xeax1GVeu","colab_type":"code","outputId":"02669242-2e50-4de2-fe55-0f0b997f602b","executionInfo":{"status":"ok","timestamp":1581352553709,"user_tz":240,"elapsed":1089,"user":{"displayName":"Yeh Ken Huai-Che","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAf2gHx8A9v2eLa5kx_yhYDpU-RovU0raLpp45QKQ=s64","userId":"07065281398709988421"}},"colab":{"base_uri":"https://localhost:8080/","height":257}},"source":["# read data prepared by kaggle into pandas' data frames\n","df_train = pd.read_csv(\"/content/train.csv\")\n","#df_test = pd.read_csv(\"/content/test.csv\")\n","#submission = pd.read_csv(\"/content/sample_submission.csv\") #no need\n","df_train, df_test = train_test_split(df_train,\n","                                     test_size=0.1,\n","                                     random_state=39)\n","\n","display(df_train.head())\n","\n","print('an example of a tweet')\n","display(df_train.text.iloc[0])"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4933</th>\n","      <td>7027</td>\n","      <td>mayhem</td>\n","      <td>PG County, MD</td>\n","      <td>Tonight It's Going To Be Mayhem @ #4PlayThursd...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1673</th>\n","      <td>2416</td>\n","      <td>collide</td>\n","      <td>Kansas, The Free State! ~ KC</td>\n","      <td>That sounds about right. Our building will hav...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5654</th>\n","      <td>8066</td>\n","      <td>rescue</td>\n","      <td>Big NorthEast Litter Box</td>\n","      <td>I'm on 2 blood pressure meds and it's still pr...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3532</th>\n","      <td>5049</td>\n","      <td>eyewitness</td>\n","      <td>Pennsylvania</td>\n","      <td>A true #TBT  Eyewitness News WBRE WYOU http://...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5212</th>\n","      <td>7444</td>\n","      <td>obliterated</td>\n","      <td>NaN</td>\n","      <td>I think I'll get obliterated tonight</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        id  ... target\n","4933  7027  ...      0\n","1673  2416  ...      0\n","5654  8066  ...      0\n","3532  5049  ...      0\n","5212  7444  ...      0\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["an example of a tweet\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["\"Tonight It's Going To Be Mayhem @ #4PlayThursdays. Everybody Free w/ Text. 1716 I ST NW (18+) http://t.co/cQ7jJ6Yjfz\""]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"mpu7cw42e8xa","colab_type":"text"},"source":["\n","\n","\n","BERT makes predictions based on the text content in the 'text' column, and we will consider columns of 'keyword' and 'location' in the booster solution to improve BERT's predictions."]},{"cell_type":"code","metadata":{"id":"5ZMWBnosf5Cf","colab_type":"code","outputId":"d21bc4cb-61fb-41b6-bf87-37a1a71ca535","executionInfo":{"status":"ok","timestamp":1581301234311,"user_tz":300,"elapsed":295,"user":{"displayName":"Yeh Ken Huai-Che","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAf2gHx8A9v2eLa5kx_yhYDpU-RovU0raLpp45QKQ=s64","userId":"07065281398709988421"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# basic infomation\n","print('basic infomation of training data')\n","display(df_train.info())\n","print('basic infomation of test data')\n","display(df_test.info())\n","\n","# stats of data\n","print('stats of training data')\n","display(df_train.describe(include=['object']))\n","print('stats of test data')\n","display(df_test.describe(include=['object']))\n","\n","# missing values\n","print('missing values in the training data')\n","display(df_train.isnull().sum())\n","print('missing values in the test data')\n","display(df_test.isnull().sum())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["basic infomation of training data\n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 6851 entries, 4933 to 3465\n","Data columns (total 5 columns):\n","id          6851 non-null int64\n","keyword     6797 non-null object\n","location    4577 non-null object\n","text        6851 non-null object\n","target      6851 non-null int64\n","dtypes: int64(2), object(3)\n","memory usage: 321.1+ KB\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["None"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["basic infomation of test data\n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 762 entries, 7313 to 3305\n","Data columns (total 5 columns):\n","id          762 non-null int64\n","keyword     755 non-null object\n","location    503 non-null object\n","text        762 non-null object\n","target      762 non-null int64\n","dtypes: int64(2), object(3)\n","memory usage: 35.7+ KB\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["None"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["stats of training data\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>6797</td>\n","      <td>4577</td>\n","      <td>6851</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>221</td>\n","      <td>3066</td>\n","      <td>6764</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>fatalities</td>\n","      <td>USA</td>\n","      <td>11-Year-Old Boy Charged With Manslaughter of T...</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>43</td>\n","      <td>94</td>\n","      <td>9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           keyword location                                               text\n","count         6797     4577                                               6851\n","unique         221     3066                                               6764\n","top     fatalities      USA  11-Year-Old Boy Charged With Manslaughter of T...\n","freq            43       94                                                  9"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["stats of test data\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>755</td>\n","      <td>503</td>\n","      <td>762</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>212</td>\n","      <td>440</td>\n","      <td>760</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>dust%20storm</td>\n","      <td>USA</td>\n","      <td>To fight bioterrorism sir.</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>12</td>\n","      <td>10</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             keyword location                        text\n","count            755      503                         762\n","unique           212      440                         760\n","top     dust%20storm      USA  To fight bioterrorism sir.\n","freq              12       10                           3"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["missing values in the training data\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["id             0\n","keyword       54\n","location    2274\n","text           0\n","target         0\n","dtype: int64"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["missing values in the test data\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["id            0\n","keyword       7\n","location    259\n","text          0\n","target        0\n","dtype: int64"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"Xb5AvVQTKhhC","colab_type":"text"},"source":["## Text cleaning\n","* The most common type of words in oov have punctuations at the start or end. Those words doesn't have embeddings because of the trailing punctuations. Punctuations #, @, !, ?, (, ),[, ], *, %, ..., ', ., :, ; are separated from words\n","* Special characters that are attached to words are removed completely\n","* Contractions are expanded\n","* Urls are removed\n","* Character entity references are replaced with their actual symbols\n","* Typos and slang are corrected, and informal abbreviations are written in their long forms\n","* Hashtags and usernames are expanded\n","* Some words are replaced with their acronyms\n","\n","See https://raw.githubusercontent.com/whitejetyeh/NLP-with-Disaster-Tweets/master/CleanTweets.py for details."]},{"cell_type":"code","metadata":{"id":"3UhGzBkBKidj","colab_type":"code","outputId":"bb838524-059d-4e94-a711-eb7dd39e57a1","executionInfo":{"status":"ok","timestamp":1581352556715,"user_tz":240,"elapsed":4061,"user":{"displayName":"Yeh Ken Huai-Che","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAf2gHx8A9v2eLa5kx_yhYDpU-RovU0raLpp45QKQ=s64","userId":"07065281398709988421"}},"colab":{"base_uri":"https://localhost:8080/","height":107}},"source":["# clean is a handmade text cleaning function for tweets\n","#Reference: https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-full-cleaning\n","from CleanTweets import clean\n","df_train['text_cleaned'] = df_train['text'].apply(lambda s : clean(s))\n","df_test['text_cleaned'] = df_test['text'].apply(lambda s : clean(s))\n","\n","print('a tweet before cleaning')\n","display(df_train.text.iloc[0])\n","print('a tweet after cleaning')\n","display(df_train.text_cleaned.iloc[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["a tweet before cleaning\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["\"Tonight It's Going To Be Mayhem @ #4PlayThursdays. Everybody Free w/ Text. 1716 I ST NW (18+) http://t.co/cQ7jJ6Yjfz\""]},"metadata":{"tags":[]}},{"output_type":"stream","text":["a tweet after cleaning\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["'Tonight It is Going To Be Mayhem  @   # Foreplay Thursdays .  Everybody Free with Text .  1716 I ST NW  ( 18 +  )  '"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"ELAoxFXMYUS3","colab_type":"text"},"source":["# 2. finetune a pretrained BERT model\n","Here, we apply the plain BERT model. The first step is to process text data into the BERT format [token, mask, segment] with `bert_encode` and `tokenizer`. Then, the model defined in `build_model` is the original base BERT with the one node output layer determining predictions to be 1 for real disaster or 0 for non-disaster.\n","\n","This part is forked from another fine kernel on kaggle, \n","https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub."]},{"cell_type":"markdown","metadata":{"id":"QB-rgQe1p_GM","colab_type":"text"},"source":["## define model"]},{"cell_type":"code","metadata":{"id":"DLpE6T7rGVea","colab_type":"code","colab":{}},"source":["def bert_encode(texts, tokenizer, max_len=512):\n","    all_tokens = []\n","    all_masks = []\n","    all_segments = []\n","    \n","    for text in texts:\n","        text = tokenizer.tokenize(text)\n","            \n","        text = text[:max_len-2]\n","        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n","        pad_len = max_len - len(input_sequence)\n","        \n","        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n","        tokens += [0] * pad_len\n","        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n","        segment_ids = [0] * max_len\n","        \n","        all_tokens.append(tokens)\n","        all_masks.append(pad_masks)\n","        all_segments.append(segment_ids)\n","    \n","    return [np.array(all_tokens), np.array(all_masks), np.array(all_segments)]\n","\n","def build_model(bert_layer, max_len=512):\n","    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n","    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n","    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n","\n","    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n","    clf_output = sequence_output[:, 0, :]\n","    out = Dense(1, activation='sigmoid')(clf_output)\n","    \n","    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n","    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n","    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1g-IoRAKp0jo","colab_type":"text"},"source":["## initialize BERT and encode text data"]},{"cell_type":"code","metadata":{"id":"c-tB-ZYEpjx8","colab_type":"code","colab":{}},"source":["# initialize BERT with trainable weights\n","bert_layer = hub.KerasLayer(module_path, trainable=True)\n","\n","# establish tokenizer with bert_layer\n","vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n","do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n","tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n","\n","# encode cleaned text data for BERT to read\n","# the maximum text length (max_len) with BERT-base is 512\n","train_input = bert_encode(df_train.text_cleaned.values, tokenizer, max_len=160)\n","test_input = bert_encode(df_test.text_cleaned.values, tokenizer, max_len=160)\n","train_labels = df_train.target.values\n","test_labels = df_test.target.values"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_aT0HzVXGVe8","colab_type":"text"},"source":["## train model and predict"]},{"cell_type":"code","metadata":{"id":"AXuD-KO-q5EK","colab_type":"code","outputId":"c6f736d4-0c71-4f46-acf0-f3061b5e54f2","executionInfo":{"status":"ok","timestamp":1581304437189,"user_tz":300,"elapsed":2117904,"user":{"displayName":"Yeh Ken Huai-Che","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAf2gHx8A9v2eLa5kx_yhYDpU-RovU0raLpp45QKQ=s64","userId":"07065281398709988421"}},"colab":{"base_uri":"https://localhost:8080/","height":868}},"source":["%%time\n","\n","train_model = False\n","if train_model:\n","    # initialize model\n","    n_epoch = 3\n","    model = build_model(bert_layer, max_len=160)\n","    checkpoint_path = \"/content/bert_model.ckpt\"\n","    display(model.summary())\n","\n","    # start training (about 30 mins)\n","    # Create a callback that saves the model's weights\n","    cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n","                                  save_weights_only=True,\n","                                  save_best_only=True,\n","                                  verbose=1)\n","    model.fit(train_input, train_labels,\n","              validation_split=0.1,\n","              epochs=n_epoch,\n","              batch_size=16,\n","              callbacks=[cp_callback])\n","\n","    # predict df_test (validation data from train.csv)\n","    predictions = model.predict(test_input).round().astype(int)\n","    print(classification_report(test_labels, predictions, labels=[0, 1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_word_ids (InputLayer)     [(None, 160)]        0                                            \n","__________________________________________________________________________________________________\n","input_mask (InputLayer)         [(None, 160)]        0                                            \n","__________________________________________________________________________________________________\n","segment_ids (InputLayer)        [(None, 160)]        0                                            \n","__________________________________________________________________________________________________\n","keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n","                                                                 input_mask[0][0]                 \n","                                                                 segment_ids[0][0]                \n","__________________________________________________________________________________________________\n","tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 1)            1025        tf_op_layer_strided_slice[0][0]  \n","==================================================================================================\n","Total params: 335,142,914\n","Trainable params: 335,142,913\n","Non-trainable params: 1\n","__________________________________________________________________________________________________\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["None"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Train on 6165 samples, validate on 686 samples\n","Epoch 1/3\n","6160/6165 [============================>.] - ETA: 0s - loss: 0.4842 - accuracy: 0.7771\n","Epoch 00001: val_loss improved from inf to 0.43643, saving model to /content/bert_model.ckpt\n","6165/6165 [==============================] - 742s 120ms/sample - loss: 0.4844 - accuracy: 0.7771 - val_loss: 0.4364 - val_accuracy: 0.8105\n","Epoch 2/3\n","6160/6165 [============================>.] - ETA: 0s - loss: 0.3424 - accuracy: 0.8589\n","Epoch 00002: val_loss improved from 0.43643 to 0.42187, saving model to /content/bert_model.ckpt\n","6165/6165 [==============================] - 708s 115ms/sample - loss: 0.3426 - accuracy: 0.8589 - val_loss: 0.4219 - val_accuracy: 0.8090\n","Epoch 3/3\n","6160/6165 [============================>.] - ETA: 0s - loss: 0.2672 - accuracy: 0.8927\n","Epoch 00003: val_loss did not improve from 0.42187\n","6165/6165 [==============================] - 634s 103ms/sample - loss: 0.2673 - accuracy: 0.8926 - val_loss: 0.4861 - val_accuracy: 0.7930\n","              precision    recall  f1-score   support\n","\n","           0       0.85      0.87      0.86       451\n","           1       0.81      0.77      0.79       311\n","\n","    accuracy                           0.83       762\n","   macro avg       0.83      0.82      0.83       762\n","weighted avg       0.83      0.83      0.83       762\n","\n","CPU times: user 16min 53s, sys: 11min 42s, total: 28min 35s\n","Wall time: 35min 16s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cBHz6qHuAzJ1","colab_type":"text"},"source":["A fine tuned plain BERT gives predictions with accuracy 0.83 and classification report as the following. \n","$$\\begin{array}{c|c|c}&precision&recall\\\\\\hline\\\\0&0.85&0.87\\\\\\hline \\\\ 1&0.81&0.77\\end{array}$$\n","This is a fine result, but the training process is computational heavy. Afterall, even the smaller BERT_base has over 300M trainable parameters. The next model with frozen BERT can achieve similar accuracy with the training time cut in half."]},{"cell_type":"markdown","metadata":{"id":"G5uxziJWYjlx","colab_type":"text"},"source":["# 3. transfer learning with BERT\n","\n","Here, we extend the pretrained BERT_base by a simple neural network. We will fix the weights in BERT and only train the attached neural network, so the training will be much easier than finetuning the complete BERT."]},{"cell_type":"markdown","metadata":{"id":"odNS1J0VxsN2","colab_type":"text"},"source":["## define model"]},{"cell_type":"code","metadata":{"id":"4JvvlMwNkFTA","colab_type":"code","colab":{}},"source":["'''Transfer learning of bert'''\n","def build_ext_model(module_path, max_len=512):\n","    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n","    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n","    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n","\n","    # load BERT with frozen weights\n","    bert_layer = hub.KerasLayer(module_path, trainable=False)\n","\n","    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n","\n","    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n","    x = tf.keras.layers.Dropout(0.2)(x)\n","    # dense layers stacked after bert\n","    x = tf.keras.layers.Dense(400, activation=\"relu\")(x)\n","    x = tf.keras.layers.Dropout(0.1)(x)\n","    x = tf.keras.layers.Dense(200, activation=\"relu\")(x)\n","    x = tf.keras.layers.Dropout(0.1)(x)\n","    x = tf.keras.layers.Dense(100, activation=\"relu\")(x)\n","    x = tf.keras.layers.Dropout(0.1)(x)\n","    out = Dense(1, activation='sigmoid', name=\"dense_output\")(x)\n","    \n","    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n","    model.compile(Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n","    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s6ZO81fqxxYW","colab_type":"text"},"source":["## train model and predict"]},{"cell_type":"code","metadata":{"id":"D4qVJvdum5jS","colab_type":"code","outputId":"87efaf1f-0fe7-469e-9ff7-09264a7eea82","executionInfo":{"status":"ok","timestamp":1581355797056,"user_tz":240,"elapsed":1320792,"user":{"displayName":"Yeh Ken Huai-Che","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAf2gHx8A9v2eLa5kx_yhYDpU-RovU0raLpp45QKQ=s64","userId":"07065281398709988421"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["%%time\n","\n","train_model = True\n","if train_model:\n","    # initialize model\n","    n_epoch = 5\n","    model = build_ext_model(module_path, max_len=160)    \n","    checkpoint_path = \"/content/bert_ext_model.ckpt\"\n","    display(model.summary())\n","\n","    # start training (about 30 mins)\n","    # Create a callback that saves the model's weights\n","    cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n","                                  save_weights_only=True,\n","                                  save_best_only=True,\n","                                  verbose=1)\n","    model.fit(train_input, train_labels,\n","              validation_split=0.1,\n","              epochs=n_epoch,\n","              batch_size=16,\n","              callbacks=[cp_callback])\n","\n","    # predict df_test (validation data from train.csv)\n","    predictions = model.predict(test_input).round().astype(int)\n","    print(classification_report(test_labels, predictions, labels=[0, 1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"model_2\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_word_ids (InputLayer)     [(None, 160)]        0                                            \n","__________________________________________________________________________________________________\n","input_mask (InputLayer)         [(None, 160)]        0                                            \n","__________________________________________________________________________________________________\n","segment_ids (InputLayer)        [(None, 160)]        0                                            \n","__________________________________________________________________________________________________\n","keras_layer_2 (KerasLayer)      [(None, 1024), (None 335141889   input_word_ids[0][0]             \n","                                                                 input_mask[0][0]                 \n","                                                                 segment_ids[0][0]                \n","__________________________________________________________________________________________________\n","global_average_pooling1d_1 (Glo (None, 1024)         0           keras_layer_2[0][1]              \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 1024)         0           global_average_pooling1d_1[0][0] \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 400)          410000      dropout_4[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_5 (Dropout)             (None, 400)          0           dense_4[0][0]                    \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 200)          80200       dropout_5[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_6 (Dropout)             (None, 200)          0           dense_5[0][0]                    \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 100)          20100       dropout_6[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_7 (Dropout)             (None, 100)          0           dense_6[0][0]                    \n","__________________________________________________________________________________________________\n","dense_output (Dense)            (None, 1)            101         dropout_7[0][0]                  \n","==================================================================================================\n","Total params: 335,652,290\n","Trainable params: 510,401\n","Non-trainable params: 335,141,889\n","__________________________________________________________________________________________________\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["None"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Train on 6165 samples, validate on 686 samples\n","Epoch 1/5\n","6160/6165 [============================>.] - ETA: 0s - loss: 0.5984 - accuracy: 0.6972\n","Epoch 00001: val_loss improved from inf to 0.50573, saving model to /content/bert_ext_model.ckpt\n","6165/6165 [==============================] - 272s 44ms/sample - loss: 0.5982 - accuracy: 0.6972 - val_loss: 0.5057 - val_accuracy: 0.7653\n","Epoch 2/5\n","6160/6165 [============================>.] - ETA: 0s - loss: 0.4926 - accuracy: 0.7807\n","Epoch 00002: val_loss improved from 0.50573 to 0.46735, saving model to /content/bert_ext_model.ckpt\n","6165/6165 [==============================] - 253s 41ms/sample - loss: 0.4926 - accuracy: 0.7805 - val_loss: 0.4674 - val_accuracy: 0.7799\n","Epoch 3/5\n","6160/6165 [============================>.] - ETA: 0s - loss: 0.4642 - accuracy: 0.7894\n","Epoch 00003: val_loss improved from 0.46735 to 0.45765, saving model to /content/bert_ext_model.ckpt\n","6165/6165 [==============================] - 253s 41ms/sample - loss: 0.4644 - accuracy: 0.7893 - val_loss: 0.4577 - val_accuracy: 0.7828\n","Epoch 4/5\n","6160/6165 [============================>.] - ETA: 0s - loss: 0.4448 - accuracy: 0.8018\n","Epoch 00004: val_loss did not improve from 0.45765\n","6165/6165 [==============================] - 247s 40ms/sample - loss: 0.4449 - accuracy: 0.8016 - val_loss: 0.4599 - val_accuracy: 0.7901\n","Epoch 5/5\n","6160/6165 [============================>.] - ETA: 0s - loss: 0.4419 - accuracy: 0.8091\n","Epoch 00005: val_loss improved from 0.45765 to 0.44334, saving model to /content/bert_ext_model.ckpt\n","6165/6165 [==============================] - 252s 41ms/sample - loss: 0.4421 - accuracy: 0.8089 - val_loss: 0.4433 - val_accuracy: 0.8061\n","              precision    recall  f1-score   support\n","\n","           0       0.83      0.88      0.86       451\n","           1       0.81      0.74      0.78       311\n","\n","    accuracy                           0.83       762\n","   macro avg       0.82      0.81      0.82       762\n","weighted avg       0.82      0.83      0.82       762\n","\n","CPU times: user 5min 45s, sys: 4min 41s, total: 10min 27s\n","Wall time: 22min\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XMrjMfRdD32P","colab_type":"text"},"source":["A simple 3 layers neural network attached after BERT gives predictions with accuracy 0.83 and classification report as the following. \n","$$\\begin{array}{c|c|c}&precision&recall\\\\\\hline\\\\0&0.83&0.88\\\\\\hline \\\\ 1&0.81&0.74\\end{array}$$\n","This transfer learning model achieves accuracy similar to the finetuned BERT with significantly less trainable weights (300M to 0.5M).\n"]},{"cell_type":"markdown","metadata":{"id":"Yb4ASvWd47pY","colab_type":"text"},"source":["# 4. subsequent booster after BERT\n","\n","Here, we build a pipeline of the finetuned BERT following by an ensemble of several classifiers.\n","The ensemble classifier takes the input of preliminary predictions from BERT and features of keyword and location, which are not considered in BERT, and therefore the aggregated predictions can be further improved.\n","\n","The ensemble classifier averages the predictions from a histogram gradient boosting (HGB) classifier, a stochastic gradient boosting (SGD) classifier, and a gradient boosting (GB) classifier. By averaging different classifiers, the ensemble with lower variance is more adaptive to diverse data."]},{"cell_type":"markdown","metadata":{"id":"r3JWMXEov8c8","colab_type":"text"},"source":["## encode categorical features\n","\n","We need to encode 'keyword' and 'location' into number before concating them with BERT's predictions. `LabelEncoder` encodes n labels into numbers 0 to n-1, and `MinMaxScaler` scales these numbers into range [0,1]."]},{"cell_type":"code","metadata":{"id":"0JSenH5Gv7qw","colab_type":"code","colab":{}},"source":["'''encode categorical features'''\n","from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n","from collections import defaultdict\n","\n","cat_df_train = df_train[['keyword','location']].copy()\n","cat_df_test = df_test[['keyword','location']].copy()\n","# handle missing values\n","for cat in ['keyword','location']:\n","    cat_df_train[cat].loc[cat_df_train[cat].isnull()] = 'NaN'\n","    cat_df_test[cat].loc[cat_df_test[cat].isnull()] = 'NaN'\n","\n","# initialize the encoder\n","le = defaultdict(LabelEncoder)\n","# fit the encoder and transform the training set\n","fit_cat_df_train = cat_df_train.apply(lambda x: le[x.name].fit_transform(x))\n","# normalize the encoding\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","fit_cat_df_train = scaler.fit_transform(fit_cat_df_train)\n","fit_cat_df_train = pd.DataFrame(fit_cat_df_train,\n","                                index=cat_df_train.index,\n","                                columns=['keyword','location'])\n","\n","\n","# Replace test set labels unseen in the training set\n","for cat in ['keyword','location']:\n","    labels_train = cat_df_train[cat].unique().tolist()\n","    replacement_label = cat_df_train[cat].mode()[0]\n","    cat_df_test[cat].loc[~cat_df_test[cat].isin(labels_train)] = replacement_label\n","\n","# Using the dictionary to label future data\n","fit_cat_df_test = cat_df_test.apply(lambda x: le[x.name].transform(x))\n","# normalize the encoding\n","fit_cat_df_test = scaler.transform(fit_cat_df_test)\n","fit_cat_df_test = pd.DataFrame(fit_cat_df_test,\n","                               index=cat_df_test.index,\n","                               columns=['keyword','location'])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vSQWfbZ6yw-5","colab_type":"text"},"source":["## aggregate bert predictions with extra features in subsequent models\n","\n","Firstly, we concate the preliminary predictions with encoded 'keyword' and 'location'."]},{"cell_type":"code","metadata":{"id":"8dPVZ1VHy5au","colab_type":"code","outputId":"4de85b0f-104e-4c2d-97d7-1c751094d768","executionInfo":{"status":"ok","timestamp":1581344508665,"user_tz":300,"elapsed":140169,"user":{"displayName":"Yeh Ken Huai-Che","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAf2gHx8A9v2eLa5kx_yhYDpU-RovU0raLpp45QKQ=s64","userId":"07065281398709988421"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["%%time\n","'''aggregate previous predictions with extra features(processed)'''\n","\n","# preliminary prediction from BERT\n","bert_train_pred = model.predict(train_input)\n","bert_predict_df = pd.DataFrame(bert_train_pred, \n","                               index=df_train.index,\n","                               columns=['target'])\n","# more features to be considered\n","boosting_input = pd.concat([bert_predict_df,fit_cat_df_train],axis=1)\n","    \n","bert_test_pred = model.predict(test_input)\n","bert_test_predict_df = pd.DataFrame(bert_test_pred, \n","                                    index=df_test.index,\n","                                    columns=['target'])\n","# more features to be considered\n","test_boosting_input = pd.concat([bert_test_predict_df,fit_cat_df_test],axis=1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 1min 22s, sys: 57.2 s, total: 2min 20s\n","Wall time: 2min 19s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y92OYDuoU8Jx","colab_type":"text"},"source":["Secondly, we indivisually examinate the improvements from each classifier."]},{"cell_type":"code","metadata":{"id":"V7POrWiT_AJj","colab_type":"code","outputId":"854e1705-177b-4fe5-eb9f-7571f4e4fea1","executionInfo":{"status":"ok","timestamp":1581345040181,"user_tz":300,"elapsed":582,"user":{"displayName":"Yeh Ken Huai-Che","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAf2gHx8A9v2eLa5kx_yhYDpU-RovU0raLpp45QKQ=s64","userId":"07065281398709988421"}},"colab":{"base_uri":"https://localhost:8080/","height":335}},"source":["%%time\n","'''HistGradientBoosting Classifier (lightGBM inspired)'''\n","# To use this experimental feature, we need to explicitly ask for it:\n","from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n","from sklearn.ensemble import HistGradientBoostingClassifier\n","HGB_classifier = HistGradientBoostingClassifier(l2_regularization=0.5, \n","                                                learning_rate=0.1,\n","                                                max_depth=None,\n","                                                max_iter=100, \n","                                                max_leaf_nodes=31,\n","                                                min_samples_leaf=20)\n","HGB_classifier.fit(boosting_input, train_labels)\n","\n","predictions = HGB_classifier.predict(test_boosting_input).round().astype(int)\n","print(classification_report(test_labels, predictions, labels=[0, 1]))\n","print(HGB_classifier)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.85      0.89      0.87       451\n","           1       0.82      0.77      0.80       311\n","\n","    accuracy                           0.84       762\n","   macro avg       0.84      0.83      0.83       762\n","weighted avg       0.84      0.84      0.84       762\n","\n","HistGradientBoostingClassifier(l2_regularization=0.5, learning_rate=0.1,\n","                               loss='auto', max_bins=255, max_depth=None,\n","                               max_iter=100, max_leaf_nodes=31,\n","                               min_samples_leaf=20, n_iter_no_change=None,\n","                               random_state=None, scoring=None, tol=1e-07,\n","                               validation_fraction=0.1, verbose=0,\n","                               warm_start=False)\n","CPU times: user 703 ms, sys: 31.5 ms, total: 735 ms\n","Wall time: 377 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"360ECnpQiLrY","colab_type":"code","outputId":"e8c475fe-d922-4fc0-b2bd-885a0f9f7e6f","executionInfo":{"status":"ok","timestamp":1581346923714,"user_tz":300,"elapsed":718,"user":{"displayName":"Yeh Ken Huai-Che","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAf2gHx8A9v2eLa5kx_yhYDpU-RovU0raLpp45QKQ=s64","userId":"07065281398709988421"}},"colab":{"base_uri":"https://localhost:8080/","height":211}},"source":["%%time\n","from sklearn.linear_model import SGDClassifier\n","\n","sgd = SGDClassifier()\n","sgd.fit(boosting_input, train_labels)\n","\n","predictions = sgd.predict(test_boosting_input).round().astype(int)\n","print(classification_report(test_labels, predictions, labels=[0, 1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.85      0.89      0.87       451\n","           1       0.83      0.77      0.80       311\n","\n","    accuracy                           0.84       762\n","   macro avg       0.84      0.83      0.83       762\n","weighted avg       0.84      0.84      0.84       762\n","\n","CPU times: user 21.4 ms, sys: 761 µs, total: 22.2 ms\n","Wall time: 24.5 ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9FOcvWu2VRbq","colab_type":"text"},"source":["Unlike HGB and SGD, we need some extra efforts on GB to achieve the same level of accuracy. `RandomizedSearchCV` helps to tailor parameters with respect to the training data by randomly searching through the parameter grid."]},{"cell_type":"code","metadata":{"id":"kQrTwwNoikLT","colab_type":"code","outputId":"cd50f392-5821-4b0e-8ee4-4b45bacce518","executionInfo":{"status":"ok","timestamp":1581346163692,"user_tz":300,"elapsed":53167,"user":{"displayName":"Yeh Ken Huai-Che","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAf2gHx8A9v2eLa5kx_yhYDpU-RovU0raLpp45QKQ=s64","userId":"07065281398709988421"}},"colab":{"base_uri":"https://localhost:8080/","height":371}},"source":["%%time\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.metrics import classification_report\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","# parameters set up\n","estimator = GradientBoostingClassifier()\n","param_grid={'n_estimators':[100,150], \n","            'learning_rate': [0.05,0.1,0.2],\n","            'max_depth':[2,4,6], \n","            'min_samples_leaf':[3,5,7], \n","            'max_features':['sqrt','auto']\n","           }\n","n_iter=40 #test number of settings ~sqrt(choices)\n","\n","# create best estimator\n","rand_classifier = RandomizedSearchCV(estimator, param_distributions=param_grid, \n","                                    cv=3, n_iter=n_iter, n_jobs=-1, verbose=2)\n","rand_classifier.fit(boosting_input, train_labels)\n","display(rand_classifier.best_params_)\n","gb = rand_classifier.best_estimator_\n","\n","predictions = gb.predict(test_boosting_input).round().astype(int)\n","print(classification_report(test_labels, predictions, labels=[0, 1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   16.7s\n","[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   52.1s finished\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/plain":["{'learning_rate': 0.1,\n"," 'max_depth': 2,\n"," 'max_features': 'sqrt',\n"," 'min_samples_leaf': 5,\n"," 'n_estimators': 150}"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.85      0.88      0.86       451\n","           1       0.82      0.77      0.79       311\n","\n","    accuracy                           0.84       762\n","   macro avg       0.83      0.83      0.83       762\n","weighted avg       0.84      0.84      0.84       762\n","\n","CPU times: user 908 ms, sys: 81.1 ms, total: 989 ms\n","Wall time: 52.5 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X2wdGWM8rrKj","colab_type":"text"},"source":["AdaBoost algorithm slightly improves GB with the best parameters."]},{"cell_type":"code","metadata":{"id":"0Fng-Yf4jfPo","colab_type":"code","outputId":"046eee52-9489-43f2-cd0f-95e55bb115ec","executionInfo":{"status":"ok","timestamp":1581346253903,"user_tz":300,"elapsed":4363,"user":{"displayName":"Yeh Ken Huai-Che","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAf2gHx8A9v2eLa5kx_yhYDpU-RovU0raLpp45QKQ=s64","userId":"07065281398709988421"}},"colab":{"base_uri":"https://localhost:8080/","height":176}},"source":["from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","\n","estimator = GradientBoostingClassifier(n_estimators = 150,\n","                                       min_samples_leaf = 5,\n","                                       max_features = 'sqrt',\n","                                       max_depth = 2,\n","                                       learning_rate = 0.1)\n","\n","abgb = AdaBoostClassifier(base_estimator=estimator,\n","                          learning_rate=1,\n","                          n_estimators=10,\n","                          random_state=39)\n","abgb.fit(boosting_input, train_labels)\n","\n","predictions = abgb.predict(test_boosting_input).round().astype(int)\n","print(classification_report(test_labels, predictions, labels=[0, 1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.85      0.88      0.87       451\n","           1       0.82      0.78      0.80       311\n","\n","    accuracy                           0.84       762\n","   macro avg       0.84      0.83      0.83       762\n","weighted avg       0.84      0.84      0.84       762\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kFd-qKs5lqWC","colab_type":"text"},"source":["## ensemble of effective classifiers"]},{"cell_type":"code","metadata":{"id":"C6Iuha4_l3c3","colab_type":"code","outputId":"6bb874f9-b1ee-40b8-c957-7d5b1a807f10","executionInfo":{"status":"ok","timestamp":1581346893395,"user_tz":300,"elapsed":4820,"user":{"displayName":"Yeh Ken Huai-Che","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAf2gHx8A9v2eLa5kx_yhYDpU-RovU0raLpp45QKQ=s64","userId":"07065281398709988421"}},"colab":{"base_uri":"https://localhost:8080/","height":211}},"source":["%%time\n","'''voting for optimal prediction in the ensemble of effective classifiers'''\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import classification_report\n","# To use this experimental feature, we need to explicitly ask for it:\n","from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n","from sklearn.ensemble import HistGradientBoostingClassifier\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n","\n","#enter selected classifiers\n","estimator = GradientBoostingClassifier(n_estimators = 150,\n","                                       min_samples_leaf = 5,\n","                                       max_features = 'sqrt',\n","                                       max_depth = 2,\n","                                       learning_rate = 0.1)\n","ABGB = AdaBoostClassifier(base_estimator=estimator,\n","                          learning_rate=1,\n","                          n_estimators=10,\n","                          random_state=39)\n","\n","SGD = SGDClassifier()\n","\n","HGB = HistGradientBoostingClassifier(l2_regularization=0.5, \n","                                     learning_rate=0.1,\n","                                     max_depth=None,\n","                                     max_iter=100, \n","                                     max_leaf_nodes=31,\n","                                     min_samples_leaf=20)\n","\n","#ensemble of classifiers\n","ensemble = [('abgb', ABGB),                                     \n","            ('sgd', SGD),\n","            ('hgb', HGB)]\n","weights = [1, 1, 1]\n","voting_classifier = VotingClassifier(ensemble,\n","                                     weights=weights,\n","                                     n_jobs=-1)\n","\n","\n","#fit all regressors\n","voting_classifier.fit(boosting_input, train_labels)\n","\n","predictions = voting_classifier.predict(test_boosting_input).round().astype(int)\n","print(classification_report(test_labels, predictions, labels=[0, 1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.85      0.90      0.87       451\n","           1       0.84      0.76      0.80       311\n","\n","    accuracy                           0.84       762\n","   macro avg       0.84      0.83      0.83       762\n","weighted avg       0.84      0.84      0.84       762\n","\n","CPU times: user 116 ms, sys: 2.16 ms, total: 118 ms\n","Wall time: 4.3 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3qfog2v3sv4O","colab_type":"text"},"source":["The ensemble classifier attached after BERT gives predictions with accuracy 0.84 (improved from 0.83) and classification report as the following. \n","$$\\begin{array}{c|c|c}&precision&recall\\\\\\hline\\\\0&0.85&0.90\\\\\\hline \\\\ 1&0.84&0.76\\end{array}$$\n"]}]}